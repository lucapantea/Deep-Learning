#!/bin/bash

#SBATCH --partition=gpu_shared_course
#SBATCH --gres=gpu:1
#SBATCH --job-name=train-all
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=3
#SBATCH --time=04:00:00
#SBATCH --mem=32000M
#SBATCH --output=slurm_output_train-%x.out

module purge
module load 2021
module load Anaconda3/2021.05

# activate the environment
source activate ~/.conda/envs/dl2022
# specify which directory
code_dir=./

# running the training script
python $code_dir/train.py --augmentation_name 'all'
